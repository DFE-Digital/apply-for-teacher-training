name: Backup database to Azure storage

on:
  workflow_dispatch:
    inputs:
      environment:
        description: Environment to backup
        required: true
        default: qa
        type: choice
        options:
        - qa
        - staging
        - sandbox
        - production
      backup-file:
        description: |
          Backup file name (without extension). Default is att_[env]_adhoc_YYYY-MM-DD. Set it explicitly when backing up a point-in-time (PTR) server. (Optional)
        required: false
        type: string
        default: default
      db-server:
        description: |
          Name of the database server. Default is the live server. When backing up a point-in-time (PTR) server, use the full name of the PTR server. (Optional)

  schedule:
    - cron: "0 2 * * *" # 02:00 UTC

env:
  SERVICE_NAME: apply
  SERVICE_SHORT: att
  TF_VARS_PATH: terraform/aks/workspace_variables

permissions:
  id-token: write

jobs:
  backup:
    name: Backup database
    runs-on: ubuntu-latest
    services:
      postgres:
        image: postgres:14
        env:
          POSTGRES_USER: postgres
          POSTGRES_PASSWORD: postgres
          POSTGRES_DB: postgres
        ports:
          - 5432:5432
        options: --health-cmd pg_isready --health-interval 10s --health-timeout 5s --health-retries 5
    environment:
      name: ${{ inputs.environment || 'production' }}
    env:
      DEPLOY_ENV: ${{ inputs.environment || 'production'  }}
      BACKUP_FILE: ${{ inputs.backup-file || 'schedule'  }}

    steps:
    - uses: actions/checkout@v4

    - uses: azure/login@v2
      with:
        client-id: ${{ secrets.AZURE_CLIENT_ID }}
        tenant-id: ${{ secrets.AZURE_TENANT_ID }}
        subscription-id: ${{ secrets.AZURE_SUBSCRIPTION_ID }}

    - name: Set environment variables
      run: |
        source global_config/${DEPLOY_ENV}.sh
        tf_vars_file=${TF_VARS_PATH}/${DEPLOY_ENV}.tfvars.json
        echo "CLUSTER=$(jq -r '.cluster' ${tf_vars_file})" >> $GITHUB_ENV
        echo "AKS_ENV=$(jq -r '.app_environment' ${tf_vars_file})" >> $GITHUB_ENV
        echo "NAMESPACE=$(jq -r '.namespace' ${tf_vars_file})" >> $GITHUB_ENV
        echo "RESOURCE_GROUP_NAME=${RESOURCE_NAME_PREFIX}-${SERVICE_SHORT}-${CONFIG_SHORT}-rg" >> $GITHUB_ENV
        echo "STORAGE_ACCOUNT_NAME=${RESOURCE_NAME_PREFIX}${SERVICE_SHORT}dbbkp${CONFIG_SHORT}sa" >> $GITHUB_ENV
        TODAY=$(date +"%F")
        echo "DB_SERVER=${RESOURCE_NAME_PREFIX}-${SERVICE_SHORT}-${CONFIG_SHORT}-psql" >> $GITHUB_ENV
        if [ "${BACKUP_FILE}" == "schedule" ]; then
          BACKUP_FILE=${SERVICE_SHORT}_${CONFIG_SHORT}_${TODAY}
        elif [ "${BACKUP_FILE}" == "default" ]; then
          BACKUP_FILE=${SERVICE_SHORT}_${CONFIG_SHORT}_adhoc_${TODAY}
        else
          BACKUP_FILE=${BACKUP_FILE}
        fi
        echo "BACKUP_FILE=${BACKUP_FILE}" >> $GITHUB_ENV

    - name: Backup ${{ env.DEPLOY_ENV }} postgres
      uses: DFE-Digital/github-actions/backup-postgres@master
      with:
        storage-account: ${{ env.STORAGE_ACCOUNT_NAME }}
        resource-group: ${{ env.RESOURCE_GROUP_NAME }}
        app-name: ${{ env.SERVICE_NAME }}-${{ env.AKS_ENV }}
        namespace: ${{ env.NAMESPACE }}
        cluster: ${{ env.CLUSTER }}
        azure-client-id: ${{ secrets.AZURE_CLIENT_ID }}
        azure-tenant-id: ${{ secrets.AZURE_TENANT_ID }}
        azure-subscription-id: ${{ secrets.AZURE_SUBSCRIPTION_ID }}
        backup-file: ${{ env.BACKUP_FILE }}.sql
        db-server-name: ${{ inputs.db-server }}
        slack-webhook: ${{ secrets.SLACK_WEBHOOK }}

    - name: Disk cleanup
      if: github.event_name == 'schedule'
      shell: bash
      run: |
        sudo rm -rf /usr/local/lib/android || true
        sudo rm -rf /usr/share/dotnet || true
        sudo rm -rf /opt/ghc || true
        sudo rm -rf /usr/local/.ghcup || true
        sudo rm -rf /opt/hostedtoolcache/CodeQL || true
        sudo rm -rf /usr/local/share/boost || true
        sudo docker image prune --all --force || true
        sudo apt-get remove -y '^aspnetcore-.*' || true
        sudo apt-get remove -y '^dotnet-.*' --fix-missing || true
        sudo apt-get remove -y '^llvm-.*' --fix-missing || true
        sudo apt-get remove -y 'php.*' --fix-missing || true
        sudo apt-get remove -y '^mongodb-.*' --fix-missing || true
        sudo apt-get remove -y '^mysql-.*' --fix-missing || true
        sudo apt-get remove -y google-chrome-stable firefox powershell mono-devel libgl1-mesa-dri --fix-missing || true
        sudo apt-get remove -y google-cloud-sdk --fix-missing || true
        sudo apt-get remove -y google-cloud-cli --fix-missing || true
        sudo rm -rf "$AGENT_TOOLSDIRECTORY"/PyPy || true
        sudo rm -rf "$AGENT_TOOLSDIRECTORY"/Python || true
        sudo rm -rf "$AGENT_TOOLSDIRECTORY"/go || true
        sudo rm -rf "$AGENT_TOOLSDIRECTORY"/node || true
        sudo apt-get autoremove -y || true
        sudo apt-get clean

    - name: Sanitise dump (optimized)
      if: github.event_name == 'schedule'
      run: |
        # Create a working directory
        mkdir -p /tmp/pg_work
        cd /tmp/pg_work

        # 1. EXTRACT AND ANALYZE SANITIZE SCRIPT TO IDENTIFY EXCLUDED TABLES
        echo "Analyzing sanitize script to identify excluded tables..."

        # Extract tables that get dropped or truncated in sanitise.sql
        EXCLUDED_TABLES=$(grep -E '(DROP TABLE|TRUNCATE TABLE)' $GITHUB_WORKSPACE/db/scripts/sanitise.sql |
          grep -oE '[a-zA-Z0-9_]+;' | sed 's/;//' | sort | uniq | tr '\n' ',')
        echo "Tables that will be excluded: $EXCLUDED_TABLES"

        # 2. CREATE EMPTY DATABASE WITH SCHEMA ONLY
        echo "Creating database with schema only..."
        createdb ${DATABASE_NAME}
        gzip -d --to-stdout $GITHUB_WORKSPACE/${{ env.BACKUP_FILE }}.sql.gz |
          pg_dump -f schema_only.sql --schema-only -d postgres
        psql -d ${DATABASE_NAME} -f schema_only.sql

        # 3. GET LIST OF TABLES TO LOAD (EXCLUDING DROPPED TABLES)
        echo "Creating list of tables to load data for..."
        psql -d ${DATABASE_NAME} -t -c "SELECT tablename FROM pg_tables WHERE schemaname='public'" |
          grep -v '^\s*$' > all_tables.txt

        # Create a filtered list excluding tables that will be dropped or truncated
        cat all_tables.txt | grep -v -E "$(echo $EXCLUDED_TABLES | sed 's/,/|/g')" > tables_to_load.txt

        # 4. LOAD DATA ONLY FOR TABLES WE NEED TO KEEP
        echo "Loading data for required tables only..."
        while read table; do
          echo "Loading data for table: $table"

          # Extract and load just this table's data
          gzip -d --to-stdout $GITHUB_WORKSPACE/${{ env.BACKUP_FILE }}.sql.gz |
            awk -v table="$table" '
              BEGIN { copy = 0; }
              $0 ~ "^COPY " table "(" { copy = 1; print; next; }
              $0 ~ "^COPY " table " " { copy = 1; print; next; }
              $0 ~ "^\\\\.$" && copy == 1 { copy = 0; print; next; }
              copy == 1 { print; }
            ' > "${table}_data.sql"

          # Load if data exists
          if [ -s "${table}_data.sql" ]; then
            psql -d ${DATABASE_NAME} -f "${table}_data.sql"
          fi

          rm "${table}_data.sql"
        done < tables_to_load.txt

        # 5. RUN SANITIZE SCRIPT (WILL NOW BE FASTER WITH LESS DATA)
        echo "Running sanitization script..."
        cd $GITHUB_WORKSPACE
        psql -d ${DATABASE_NAME} -f db/scripts/sanitise.sql

        # 6. CREATE FINAL BACKUP
        echo "Creating sanitized backup..."
        pg_dump --encoding utf8 --compress=1 --clean --no-owner --if-exists -d ${DATABASE_NAME} -f att_backup_sanitised.sql.gz

        # 7. CLEANUP
        rm -rf /tmp/pg_work
        rm ${{ env.BACKUP_FILE }}.sql.gz
      env:
        DATABASE_NAME: apply_manage_itt
        PGUSER: postgres
        PGPASSWORD: postgres
        PGHOST: localhost
        PGPORT: 5432

    - name: Upload sanitized backup to Azure storage
      if: github.event_name == 'schedule'
      run: |
        STORAGE_CONN_STR=$(az storage account show-connection-string -g ${{ env.RESOURCE_GROUP_NAME }} -n ${{ env.STORAGE_ACCOUNT_NAME }} --query 'connectionString')
        echo "::add-mask::$STORAGE_CONN_STR"
        echo "STORAGE_CONN_STR=$STORAGE_CONN_STR" >> $GITHUB_ENV
        az storage blob upload --container-name database-backup \
        --file att_backup_sanitised.sql.gz --name att_backup_sanitised.sql.gz --overwrite \
        --connection-string '${{ env.STORAGE_CONN_STR }}'
        rm att_backup_sanitised.sql.gz
