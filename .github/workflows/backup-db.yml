name: Backup database to Azure storage

on:
  workflow_dispatch:
    inputs:
      environment:
        description: Environment to backup
        required: true
        default: qa
        type: choice
        options:
        - qa
        - staging
        - sandbox
        - production
      backup-file:
        description: |
          Backup file name (without extension). Default is att_[env]_adhoc_YYYY-MM-DD. Set it explicitly when backing up a point-in-time (PTR) server. (Optional)
        required: false
        type: string
        default: default
      db-server:
        description: |
          Name of the database server. Default is the live server. When backing up a point-in-time (PTR) server, use the full name of the PTR server. (Optional)

  schedule:
    - cron: "0 2 * * *" # 02:00 UTC

env:
  SERVICE_NAME: apply
  SERVICE_SHORT: att
  TF_VARS_PATH: terraform/aks/workspace_variables

permissions:
  id-token: write

jobs:
  backup:
    name: Backup database
    runs-on: ubuntu-latest
    services:
      postgres:
        image: postgres:14
        env:
          POSTGRES_USER: postgres
          POSTGRES_PASSWORD: postgres
          POSTGRES_DB: postgres
        ports:
          - 5432:5432
        options: --health-cmd pg_isready --health-interval 10s --health-timeout 5s --health-retries 5
    environment:
      name: ${{ inputs.environment || 'production' }}
    env:
      DEPLOY_ENV: ${{ inputs.environment || 'production'  }}
      BACKUP_FILE: ${{ inputs.backup-file || 'schedule'  }}

    steps:
    - uses: actions/checkout@v4

    - uses: azure/login@v2
      with:
        client-id: ${{ secrets.AZURE_CLIENT_ID }}
        tenant-id: ${{ secrets.AZURE_TENANT_ID }}
        subscription-id: ${{ secrets.AZURE_SUBSCRIPTION_ID }}

    - name: Set environment variables
      run: |
        source global_config/${DEPLOY_ENV}.sh
        tf_vars_file=${TF_VARS_PATH}/${DEPLOY_ENV}.tfvars.json
        echo "CLUSTER=$(jq -r '.cluster' ${tf_vars_file})" >> $GITHUB_ENV
        echo "AKS_ENV=$(jq -r '.app_environment' ${tf_vars_file})" >> $GITHUB_ENV
        echo "NAMESPACE=$(jq -r '.namespace' ${tf_vars_file})" >> $GITHUB_ENV
        echo "RESOURCE_GROUP_NAME=${RESOURCE_NAME_PREFIX}-${SERVICE_SHORT}-${CONFIG_SHORT}-rg" >> $GITHUB_ENV
        echo "STORAGE_ACCOUNT_NAME=${RESOURCE_NAME_PREFIX}${SERVICE_SHORT}dbbkp${CONFIG_SHORT}sa" >> $GITHUB_ENV
        TODAY=$(date +"%F")
        echo "DB_SERVER=${RESOURCE_NAME_PREFIX}-${SERVICE_SHORT}-${CONFIG_SHORT}-psql" >> $GITHUB_ENV
        if [ "${BACKUP_FILE}" == "schedule" ]; then
          BACKUP_FILE=${SERVICE_SHORT}_${CONFIG_SHORT}_${TODAY}
        elif [ "${BACKUP_FILE}" == "default" ]; then
          BACKUP_FILE=${SERVICE_SHORT}_${CONFIG_SHORT}_adhoc_${TODAY}
        else
          BACKUP_FILE=${BACKUP_FILE}
        fi
        echo "BACKUP_FILE=${BACKUP_FILE}" >> $GITHUB_ENV

    - name: Backup ${{ env.DEPLOY_ENV }} postgres
      uses: DFE-Digital/github-actions/backup-postgres@master
      with:
        storage-account: ${{ env.STORAGE_ACCOUNT_NAME }}
        resource-group: ${{ env.RESOURCE_GROUP_NAME }}
        app-name: ${{ env.SERVICE_NAME }}-${{ env.AKS_ENV }}
        namespace: ${{ env.NAMESPACE }}
        cluster: ${{ env.CLUSTER }}
        azure-client-id: ${{ secrets.AZURE_CLIENT_ID }}
        azure-tenant-id: ${{ secrets.AZURE_TENANT_ID }}
        azure-subscription-id: ${{ secrets.AZURE_SUBSCRIPTION_ID }}
        backup-file: ${{ env.BACKUP_FILE }}.sql
        db-server-name: ${{ inputs.db-server }}
        slack-webhook: ${{ secrets.SLACK_WEBHOOK }}

    - name: Disk cleanup
      if: github.event_name == 'schedule' || github.event_name == 'workflow_dispatch'
      shell: bash
      run: |
        sudo rm -rf /usr/local/lib/android || true
        sudo rm -rf /usr/share/dotnet || true
        sudo rm -rf /opt/ghc || true
        sudo rm -rf /usr/local/.ghcup || true
        sudo rm -rf /opt/hostedtoolcache/CodeQL || true
        sudo rm -rf /usr/local/share/boost || true
        sudo docker image prune --all --force || true
        sudo apt-get remove -y '^aspnetcore-.*' || true
        sudo apt-get remove -y '^dotnet-.*' --fix-missing || true
        sudo apt-get remove -y '^llvm-.*' --fix-missing || true
        sudo apt-get remove -y 'php.*' --fix-missing || true
        sudo apt-get remove -y '^mongodb-.*' --fix-missing || true
        sudo apt-get remove -y '^mysql-.*' --fix-missing || true
        sudo apt-get remove -y google-chrome-stable firefox powershell mono-devel libgl1-mesa-dri --fix-missing || true
        sudo apt-get remove -y google-cloud-sdk --fix-missing || true
        sudo apt-get remove -y google-cloud-cli --fix-missing || true
        sudo rm -rf "$AGENT_TOOLSDIRECTORY"/PyPy || true
        sudo rm -rf "$AGENT_TOOLSDIRECTORY"/Python || true
        sudo rm -rf "$AGENT_TOOLSDIRECTORY"/go || true
        sudo rm -rf "$AGENT_TOOLSDIRECTORY"/node || true
        sudo apt-get autoremove -y || true
        sudo apt-get clean

    - name: Sanitise dump
      if: github.event_name == 'schedule' || github.event_name == 'workflow_dispatch'
      run: |
        # Create a working directory
        mkdir -p /tmp/pg_work
        cd /tmp/pg_work

        # 1. EXTRACT AND ANALYZE SANITIZE SCRIPT TO IDENTIFY EXCLUDED TABLES
        echo "Analyzing sanitize script to identify excluded tables..."
        echo "Content of sanitise.sql:"
        cat $GITHUB_WORKSPACE/db/scripts/sanitise.sql

        # More forgiving pattern matching for excluded tables
        EXCLUDED_TABLES=$(grep -iE '(DROP TABLE|TRUNCATE TABLE|DELETE FROM)' $GITHUB_WORKSPACE/db/scripts/sanitise.sql |
          grep -oE '[a-zA-Z0-9_]+' | sort | uniq | tr '\n' ',' || echo "")

        if [ -z "$EXCLUDED_TABLES" ]; then
          echo "Warning: No excluded tables found. This might be an error."
          echo "Proceeding with full schema extraction..."
        else
          echo "Tables that will be excluded: $EXCLUDED_TABLES"
        fi

        # 2. CREATE EMPTY DATABASE AND EXTRACT SCHEMA IN CORRECT ORDER
        echo "Creating database with schema only..."
        createdb ${DATABASE_NAME}

        echo "Extracting schema components..."

        # Extract schema in proper order with error checking
        echo "-- Extensions" > schema_only.sql
        echo "Extracting extensions..."
        gzip -d --to-stdout $GITHUB_WORKSPACE/${{ env.BACKUP_FILE }}.sql.gz |
          grep -E '^CREATE EXTENSION' >> schema_only.sql || true

        echo "Extracting types..."
        echo -e "\n-- Types" >> schema_only.sql
        gzip -d --to-stdout $GITHUB_WORKSPACE/${{ env.BACKUP_FILE }}.sql.gz |
          grep -E '^CREATE TYPE' >> schema_only.sql || true

        echo "Extracting tables..."
        echo -e "\n-- Tables" >> schema_only.sql
        gzip -d --to-stdout $GITHUB_WORKSPACE/${{ env.BACKUP_FILE }}.sql.gz |
          grep -E '^CREATE TABLE' >> schema_only.sql || true

        echo "Extracting functions..."
        echo -e "\n-- Functions" >> schema_only.sql
        gzip -d --to-stdout $GITHUB_WORKSPACE/${{ env.BACKUP_FILE }}.sql.gz |
          sed -n '/^CREATE OR REPLACE FUNCTION/,/^\$\$/p' >> schema_only.sql || true

        echo "Extracting indexes..."
        echo -e "\n-- Indexes" >> schema_only.sql
        gzip -d --to-stdout $GITHUB_WORKSPACE/${{ env.BACKUP_FILE }}.sql.gz |
          grep -E '^CREATE (UNIQUE )?INDEX' >> schema_only.sql || true

        echo "Extracting constraints..."
        echo -e "\n-- Constraints" >> schema_only.sql
        gzip -d --to-stdout $GITHUB_WORKSPACE/${{ env.BACKUP_FILE }}.sql.gz |
          grep -E '^ALTER TABLE.*ADD (CONSTRAINT|PRIMARY KEY|FOREIGN KEY)' >> schema_only.sql || true

        echo "Schema extraction complete. Contents of schema_only.sql:"
        head -n 20 schema_only.sql

        # Apply schema
        echo "Applying schema..."
        psql -d ${DATABASE_NAME} -f schema_only.sql

        # 3. GET LIST OF TABLES TO LOAD (EXCLUDING DROPPED TABLES)
        echo "Creating list of tables to load data for..."
        psql -d ${DATABASE_NAME} -t -c "SELECT tablename FROM pg_tables WHERE schemaname='public'" |
          grep -v '^\s*$' > all_tables.txt

        echo "Total tables found: $(wc -l < all_tables.txt)"

        # Create a filtered list excluding tables that will be dropped or truncated
        if [ -n "$EXCLUDED_TABLES" ]; then
          cat all_tables.txt | grep -v -E "$(echo $EXCLUDED_TABLES | sed 's/,/|/g')" > tables_to_load.txt
          echo "Tables to load after filtering: $(wc -l < tables_to_load.txt)"
        else
          # If no excluded tables were found, load all tables
          cp all_tables.txt tables_to_load.txt
          echo "Loading all tables since no exclusions were found"
        fi

        # 4. LOAD DATA ONLY FOR TABLES WE NEED TO KEEP
        echo "Loading data for required tables only..."

        # Track available disk space
        echo "Disk space before loading table data:"
        df -h /

        # Process tables with disk space monitoring
        while read table; do
          echo "Loading data for table: $table"

          # Extract and load just this table's data
          gzip -d --to-stdout $GITHUB_WORKSPACE/${{ env.BACKUP_FILE }}.sql.gz |
            awk -v table="$table" '
              BEGIN { copy = 0; }
              $0 ~ "^COPY " table "(" { copy = 1; print; next; }
              $0 ~ "^COPY " table " " { copy = 1; print; next; }
              $0 ~ "^\\\\.$" && copy == 1 { copy = 0; print; next; }
              copy == 1 { print; }
            ' > "${table}_data.sql"

          # Load if data exists
          if [ -s "${table}_data.sql" ]; then
            echo "Loading data for $table ($(wc -l < ${table}_data.sql) lines)"
            psql -d ${DATABASE_NAME} -f "${table}_data.sql"
          else
            echo "No data found for table: $table"
          fi

          # Free up space immediately
          rm "${table}_data.sql"

          # Check disk space periodically
          if [ $((RANDOM % 10)) -eq 0 ]; then
            echo "Current disk space:"
            df -h /
          fi
        done < tables_to_load.txt

        # 5. RUN SANITIZE SCRIPT
        echo "Running sanitization..."
        cd $GITHUB_WORKSPACE
        psql -d ${DATABASE_NAME} -f db/scripts/sanitise.sql

        # 6. CREATE FINAL BACKUP
        echo "Creating sanitized backup..."
        pg_dump --encoding utf8 --compress=1 --clean --no-owner --if-exists -d ${DATABASE_NAME} -f att_backup_sanitised.sql.gz

        # 7. CLEANUP
        echo "Cleaning up temporary files..."
        rm -rf /tmp/pg_work
        rm ${{ env.BACKUP_FILE }}.sql.gz

        echo "Sanitization complete!"
      env:
        DATABASE_NAME: apply_manage_itt
        PGUSER: postgres
        PGPASSWORD: postgres
        PGHOST: localhost
        PGPORT: 5432

    - name: Upload sanitized backup to Azure storage
      if: github.event_name == 'schedule' || github.event_name == 'workflow_dispatch'
      run: |
        STORAGE_CONN_STR=$(az storage account show-connection-string -g ${{ env.RESOURCE_GROUP_NAME }} -n ${{ env.STORAGE_ACCOUNT_NAME }} --query 'connectionString')
        echo "::add-mask::$STORAGE_CONN_STR"
        echo "STORAGE_CONN_STR=$STORAGE_CONN_STR" >> $GITHUB_ENV
        az storage blob upload --container-name database-backup \
        --file att_backup_sanitised.sql.gz --name att_backup_sanitised.sql.gz --overwrite \
        --connection-string '${{ env.STORAGE_CONN_STR }}'
        rm att_backup_sanitised.sql.gz
